# Rubrics as an Attack Surface: Stealthy Preference Drift in LLM Judges

![Teaser](/assets/rubrics_teaser.jpg "Teaser")
 
This repository implements a study of **Rubric-Induced Preference Drift (RIPD)** in LLM-based evaluation and alignment pipelines. We show that rubric edits which pass benchmark validation can still induce systematic, directional preference shifts on target domains that are difficult to detect with standard metrics. We further demonstrate rubric-based preference attacks and show how the resulting bias propagates through downstream post-training, leading to persistent policy misalignment.


> ðŸš§ **This repository is currently under active development.** ðŸš§


## Set Up

1. Clone Rubrics-as-an-Attack-Surface repository.
```bash
    git clone https://github.com/ruomengd/Rubrics-as-an-Attack-Surface.git
    cd Rubrics-as-an-Attack-Surface
```

2. Create the environment.

```bash
    conda create -n rubrics python=3.9
    conda activate rubrics
    pip install -r requirements.txt
```

## Data

We use five human-preference datasets (UltraFeedback, ChatbotArena, RMB, Anthropic hh-rlhf, PKU-SafeRLHF) to construct four benchmarkâ€“target settings: **Ultra-Real** and **Ultra-Creative** for helpfulness (UltraFeedback â†’ ChatbotArena), and **SafeRLHFâ€“RMB** and **Anthropicâ€“SafeRLHF** for harmlessness. All data is converted to a uniform pairwise preference format; benchmarks enforce rubric preservation, while targets measure deployment-relevant preference drift, with downstream policy experiments on Ultra-Real and Anthropicâ€“SafeRLHF.

### Scripts

The full data pipeline (download, preprocessing, filtering, and domain splitting) is run with:

```bash
sh ./scripts/dataset.sh
```

### Directory Structure

After the pipeline completes, the directory layout is:

```text
data/
â”œâ”€â”€ helpfulness/
â”‚   â”œâ”€â”€ Ultra-Real/
â”‚   â”‚   â”œâ”€â”€ Ultra-Real-Bench/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ val.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ test.jsonl
â”‚   â”‚   â””â”€â”€ Ultra-Real-Target/
â”‚   â”‚       â”œâ”€â”€ train.jsonl
â”‚   â”‚       â”œâ”€â”€ val.jsonl
â”‚   â”‚       â””â”€â”€ test.jsonl
â”‚   â””â”€â”€ ...
â”œâ”€â”€ harmlessness/
â”‚   â”œâ”€â”€ Anthropic-SafeRLHF/
â”‚   â”‚   â”œâ”€â”€ Anthropic-SafeRLHF-Bench/
â”‚   â”‚   â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”‚   â”‚   â”œâ”€â”€ val.jsonl
â”‚   â”‚   â”‚   â””â”€â”€ test.jsonl
â”‚   â”‚   â””â”€â”€ Anthropic-SafeRLHF-Target/
â”‚   â”‚       â”œâ”€â”€ train.jsonl
â”‚   â”‚       â”œâ”€â”€ val.jsonl
â”‚   â”‚       â””â”€â”€ test.jsonl
â”‚   â””â”€â”€ ...
```

**Bench vs. Target.**  
For each dataset configuration, **Bench** denotes the *benchmark domain* used during rubric development, while **Target** denotes a *held-out deployment domain* used to evaluate generalization and preference drift. Rubric edits are validated exclusively on the Bench domain and never optimized using Target data.

**Data splits and usage.**
- **train.jsonl**: Used for *rubric search and refinement*.
- **val.jsonl**: Used for *rubric selection*, ensuring benchmark compliance.
- **test.jsonl**: Used exclusively for *evaluation of Rubric-Induced Preference Drift (RIPD)* and is never accessed during rubric editing.



## Biased Rubric Search

The rubric search code lives under `rubrics_search/search/` and implements a population-based evolutionary procedure to find **benchmark-preserving** but **target-biased** rubric variants.



1. Run evolutionary search to generate candidate rubrics with `main.py`.
2. Select top-k per generation with `select_rubrics.py`.
3. Evaluate selected rubrics on **target-val** (measure induced drift) for later selection.
4. Evaluate selected rubrics from **target-val** with `select_final.py` for later selection.

To run the full rubric-search pipeline:

```bash
sh ./scripts/rubrics_search_helpfulness.sh
sh ./scripts/rubrics_search_harmlessness.sh
```

## Rubric Selection

Rubrics are selected under a **benchmark-preserving constraint**: candidates must match or exceed the seed rubricâ€™s agreement on a held-out benchmark validation split. Among feasible candidates, we choose the rubric that **maximally degrades agreement on the target validation split**.

```bash
sh ./scripts/rubrics_selection.sh
```

To assess the transferability of optimized rubrics, we provide scripts for cross-model evaluation. To take rubrics optimized on a source model (Model A) and test their performance on a target model (Model B):

```bash
sh ./scripts/rubrics_cross_model_eval.sh
```

## Downstream Policy Misalignment Evaluation

For downstream **policy misalignment** experiments, we focus on **Ultra-Real** (helpfulness) and **Anthropicâ€“SafeRLHF** (harmlessness), training policy models directly on preference labels generated by the selected rubrics.


### DPO Training

1. **Generate preference labels** using the selected rubrics:

```bash
sh scripts/dpo_labelling.sh
```

2. Train the policy with the labeled training data:

```bash
sh scripts/dpo_train.sh
```

### Policy Evaluation

Run the evaluation pipeline (you can execute any subset of steps) via:

```bash
sh scripts/dpo_eval.sh
```

The evaluation script supports the following stages:
- Generate model responses
- Score responses (using evaluators/reward models)
- Analyze win-rates
- Select Best-of-N (BoN) responses
- Evaluate final outputs with a third-party judge

## Cite Our Work
```
TBD
```